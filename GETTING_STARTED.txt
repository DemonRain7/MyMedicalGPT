===========================================
MyMedicalGPT - ä½¿ç”¨æŒ‡å—
===========================================

âœ… é¡¹ç›®å·²åˆ›å»ºå®Œæˆï¼

ğŸ“ é¡¹ç›®ä½ç½®: 
   d:\CODE\codes\MedicalGPT\MedicalGPT\MyMedicalGPT\

ğŸ“š æ–‡æ¡£å¯¼èˆª:
   1. QUICKSTART.md      - 5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹ï¼ˆæ–°æ‰‹å¿…è¯»ï¼‰
   2. README.md          - å®Œæ•´é¡¹ç›®æ–‡æ¡£
   3. INFERENCE_GUIDE.md - æ¨ç†æŠ€æœ¯è¯¦è§£
   4. PROJECT_SUMMARY.md - é¡¹ç›®æ€»ç»“ä¸å¯¹æ¯”

ğŸš€ å¿«é€Ÿå¼€å§‹ä¸‰æ­¥èµ°:

   æ­¥éª¤1: å®‰è£…ä¾èµ–
   ----------------
   cd MyMedicalGPT
   pip install -r requirements.txt

   æ­¥éª¤2: è®­ç»ƒç¬¬ä¸€ä¸ªæ¨¡å‹ï¼ˆé€‰æ‹©ä¸€ç§ï¼‰
   ---------------------------------
   # æ–¹å¼A: åªè®­ç»ƒSFTï¼ˆæ¨èæ–°æ‰‹ï¼Œ20-30åˆ†é’Ÿï¼‰
   bash scripts/train_sft.sh
   python merge_peft_adapter.py --base_model Qwen/Qwen2.5-0.5B-Instruct --lora_model outputs-sft --output_dir my-model

   # æ–¹å¼B: å®Œæ•´æµç¨‹ï¼ˆPT+SFT+DPOï¼Œ1-2å°æ—¶ï¼‰
   bash scripts/run_pipeline.sh

   æ­¥éª¤3: æµ‹è¯•æ¨ç†
   ----------------
   # å‘½ä»¤è¡Œäº¤äº’
   python inference.py --base_model my-model --template_name qwen --interactive

   # æˆ–Webç•Œé¢ï¼ˆéœ€è¦å…ˆå®‰è£…ï¼špip install gradioï¼‰
   python inference_gradio.py --model_path my-model

ğŸ¯ æ ¸å¿ƒåŠŸèƒ½:

   è®­ç»ƒ:
   - âœ… å¢é‡é¢„è®­ç»ƒ (PT)
   - âœ… æœ‰ç›‘ç£å¾®è°ƒ (SFT) 
   - âœ… ç›´æ¥åå¥½ä¼˜åŒ– (DPO)
   - âœ… LoRA/QLoRAé«˜æ•ˆå¾®è°ƒ
   - âœ… å¤šå¡å¹¶è¡Œè®­ç»ƒ

   æ¨ç†:
   - âœ… å‘½ä»¤è¡Œäº¤äº’å¼å¯¹è¯
   - âœ… æ‰¹é‡æ¨ç†
   - âœ… FastAPIæœåŠ¡
   - âœ… Gradio Webç•Œé¢
   - âœ… INT8/INT4é‡åŒ–
   - âœ… Flash AttentionåŠ é€Ÿ

ğŸ“Š é¡¹ç›®ç»“æ„:

   MyMedicalGPT/
   â”œâ”€â”€ è®­ç»ƒè„šæœ¬: pretraining.py, supervised_finetuning.py, dpo_training.py
   â”œâ”€â”€ æ¨ç†è„šæœ¬: inference.py, inference_api.py, inference_gradio.py
   â”œâ”€â”€ å·¥å…·è„šæœ¬: merge_peft_adapter.py, template.py
   â”œâ”€â”€ å¯åŠ¨è„šæœ¬: scripts/*.sh
   â”œâ”€â”€ ç¤ºä¾‹æ•°æ®: data/pretrain, data/finetune, data/reward
   â””â”€â”€ æ–‡æ¡£: *.md

ğŸ’¡ å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥:

   # æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
   tensorboard --logdir outputs-sft/runs --port 6006

   # åˆå¹¶LoRAæƒé‡
   python merge_peft_adapter.py --base_model <base> --lora_model <lora> --output_dir <output>

   # APIæœåŠ¡éƒ¨ç½²
   python inference_api.py --model_path my-model --port 8000

   # æ‰¹é‡æ¨ç†
   python inference.py --base_model my-model --data_file input.jsonl --output_file output.jsonl

ğŸ”¥ é«˜çº§æ¨ç†æ–¹æ¡ˆ:

   # vLLMé«˜æ€§èƒ½éƒ¨ç½²ï¼ˆéœ€è¦å…ˆå®‰è£…ï¼špip install vllmï¼‰
   python -m vllm.entrypoints.openai.api_server --model my-model --port 8000

   # INT8é‡åŒ–æ¨ç†
   python inference.py --base_model my-model --load_in_8bit

â“ é‡åˆ°é—®é¢˜ï¼Ÿ

   1. æ˜¾å­˜ä¸è¶³: å‡å°batch_sizeï¼Œå¯ç”¨gradient_checkpointing
   2. è®­ç»ƒå¤ªæ…¢: å‡å°æ•°æ®é‡ï¼Œå‡å°‘epochï¼Œä½¿ç”¨æ›´å°æ¨¡å‹
   3. æ•ˆæœä¸å¥½: å¢åŠ æ•°æ®é‡ï¼Œæé«˜è®­ç»ƒè½®æ•°ï¼Œæ£€æŸ¥æ•°æ®è´¨é‡
   4. æ›´å¤šFAQ: æŸ¥çœ‹README.md

ğŸ“ è·å–å¸®åŠ©:

   - HuggingFaceæ–‡æ¡£: https://huggingface.co/docs
   - Transformersæ•™ç¨‹: https://huggingface.co/course

ç¥ä½ è®­ç»ƒæ„‰å¿«ï¼ğŸ‰
