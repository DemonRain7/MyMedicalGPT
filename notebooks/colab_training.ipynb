{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyMedicalGPT - Colab Training Pipeline\n",
    "\n",
    "è¿™ä¸ªNotebookå¸®åŠ©ä½ åœ¨Google Colabä¸Šè®­ç»ƒè‡ªå·±çš„å¤§è¯­è¨€æ¨¡å‹\n",
    "\n",
    "**ä½¿ç”¨æ­¥éª¤**ï¼š\n",
    "1. é…ç½®Colab GPU: `ä»£ç æ‰§è¡Œç¨‹åº â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ GPU â†’ T4`\n",
    "2. ä¾æ¬¡è¿è¡Œä¸‹é¢çš„ä»£ç å—\n",
    "3. è®­ç»ƒå®Œæˆåä¸‹è½½æ¨¡å‹æƒé‡\n",
    "\n",
    "**è®­ç»ƒé˜¶æ®µ**ï¼š\n",
    "- Stage 1: é¢„è®­ç»ƒ (PT) - å¯é€‰\n",
    "- Stage 2: æœ‰ç›‘ç£å¾®è°ƒ (SFT) - å¿…é¡»\n",
    "- Stage 3: ç›´æ¥åå¥½ä¼˜åŒ– (DPO) - å¯é€‰\n",
    "\n",
    "**é¢„è®¡æ—¶é—´**ï¼š\n",
    "- ä»…SFT: 20-30åˆ†é’Ÿ\n",
    "- å®Œæ•´æµç¨‹: 1-2å°æ—¶\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Part 1: ç¯å¢ƒé…ç½®\n",
    "\n",
    "é¦–æ¬¡è¿è¡Œéœ€è¦å®‰è£…ä¾èµ–ï¼Œçº¦éœ€5åˆ†é’Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–¹å¼1: ä»GitHubå…‹éš†é¡¹ç›®ï¼ˆæ¨èï¼‰\n",
    "# ä¿®æ”¹ä¸ºä½ è‡ªå·±çš„ä»“åº“åœ°å€\n",
    "!git clone https://github.com/DemonRain7/MyMedicalGPT.git\n",
    "%cd MyMedicalGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–¹å¼2: ä»Google DriveåŠ è½½ï¼ˆå¦‚æœä½ å·²ç»ä¸Šä¼ äº†é¡¹ç›®ï¼‰\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/MyMedicalGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–\n",
    "!pip install -q transformers>=4.49.0 datasets peft trl accelerate loguru tensorboard\n",
    "\n",
    "print(\"âœ“ ä¾èµ–å®‰è£…å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹é¡¹ç›®ç»“æ„\n",
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Part 2: æ•°æ®å‡†å¤‡\n",
    "\n",
    "æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹ç¤ºä¾‹æ•°æ®\n",
    "print(\"=== é¢„è®­ç»ƒæ•°æ® ===\")\n",
    "!ls -lh data/pretrain/\n",
    "\n",
    "print(\"\\n=== SFTæ•°æ® ===\")\n",
    "!ls -lh data/finetune/\n",
    "\n",
    "print(\"\\n=== DPOæ•°æ® ===\")\n",
    "!ls -lh data/reward/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹SFTæ•°æ®æ ¼å¼ç¤ºä¾‹\n",
    "!head -n 2 data/finetune/medical_sft_1K_format.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“¤ ä¸Šä¼ è‡ªå·±çš„æ•°æ®ï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "å¦‚æœä½ æƒ³ä½¿ç”¨è‡ªå·±çš„æ•°æ®ï¼Œè¿è¡Œä¸‹é¢çš„ä»£ç å—ä¸Šä¼ æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸Šä¼ è‡ªå·±çš„è®­ç»ƒæ•°æ®\n",
    "from google.colab import files\n",
    "\n",
    "print(\"è¯·ä¸Šä¼ ä½ çš„è®­ç»ƒæ•°æ®ï¼ˆ.jsonlæ ¼å¼ï¼ŒShareGPTæ ¼å¼ï¼‰\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# ç§»åŠ¨åˆ°dataç›®å½•\n",
    "for filename in uploaded.keys():\n",
    "    !mv {filename} data/finetune/\n",
    "    print(f\"âœ“ {filename} å·²ä¸Šä¼ åˆ° data/finetune/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Part 3: æ¨¡å‹è®­ç»ƒ\n",
    "\n",
    "### é€‰æ‹©è®­ç»ƒæ–¹æ¡ˆï¼š\n",
    "- **æ–¹æ¡ˆA**: ä»…SFTï¼ˆæ¨èæ–°æ‰‹ï¼Œå¿«é€Ÿï¼‰\n",
    "- **æ–¹æ¡ˆB**: PT + SFTï¼ˆæœ‰é¢†åŸŸæ•°æ®ï¼‰\n",
    "- **æ–¹æ¡ˆC**: PT + SFT + DPOï¼ˆå®Œæ•´æµç¨‹ï¼‰\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ æ–¹æ¡ˆA: ä»…SFTè®­ç»ƒï¼ˆæ¨èï¼‰\n",
    "\n",
    "ç›´æ¥åœ¨åŸºåº§æ¨¡å‹ä¸ŠåšæŒ‡ä»¤å¾®è°ƒï¼Œæœ€ç®€å•å¿«é€Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: SFTè®­ç»ƒ\n",
    "# ä½¿ç”¨Qwen2.5-0.5Bä½œä¸ºåŸºåº§æ¨¡å‹ï¼ˆå°è€Œå¿«ï¼‰\n",
    "# å¦‚æœæƒ³ç”¨æ›´å¤§çš„æ¨¡å‹ï¼Œå¯ä»¥æ”¹æˆ Qwen/Qwen2.5-1.5B æˆ– Qwen/Qwen2.5-3B\n",
    "\n",
    "!python supervised_finetuning.py \\\n",
    "    --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \\\n",
    "    --train_file_dir ./data/finetune \\\n",
    "    --validation_file_dir ./data/finetune \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --use_peft True \\\n",
    "    --template_name qwen \\\n",
    "    --bf16 \\\n",
    "    --max_train_samples 1000 \\\n",
    "    --max_eval_samples 10 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --weight_decay 0.05 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 10 \\\n",
    "    --eval_steps 50 \\\n",
    "    --eval_strategy steps \\\n",
    "    --save_steps 500 \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 3 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --preprocessing_num_workers 1 \\\n",
    "    --output_dir outputs-sft \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ddp_timeout 30000 \\\n",
    "    --logging_first_step True \\\n",
    "    --target_modules all \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype bfloat16 \\\n",
    "    --device_map auto \\\n",
    "    --report_to tensorboard \\\n",
    "    --ddp_find_unused_parameters False \\\n",
    "    --gradient_checkpointing True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹è®­ç»ƒç»“æœ\n",
    "!ls -lh outputs-sft/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå¹¶LoRAæƒé‡åˆ°åŸºåº§æ¨¡å‹ï¼ˆå¯é€‰ï¼Œæ¨èç›´æ¥ä½¿ç”¨LoRAï¼‰\n",
    "!python merge_peft_adapter.py \\\n",
    "    --base_model Qwen/Qwen2.5-0.5B-Instruct \\\n",
    "    --lora_model outputs-sft \\\n",
    "    --output_dir merged-sft\n",
    "\n",
    "print(\"âœ“ æ¨¡å‹åˆå¹¶å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”„ æ–¹æ¡ˆB: PT + SFTï¼ˆæœ‰é¢†åŸŸæ•°æ®æ—¶ä½¿ç”¨ï¼‰\n",
    "\n",
    "å…ˆåœ¨é¢†åŸŸæ–‡æœ¬ä¸Šé¢„è®­ç»ƒï¼Œå†åšæŒ‡ä»¤å¾®è°ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: é¢„è®­ç»ƒ\n",
    "!python pretraining.py \\\n",
    "    --model_name_or_path Qwen/Qwen2.5-0.5B \\\n",
    "    --train_file_dir ./data/pretrain \\\n",
    "    --validation_file_dir ./data/pretrain \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --per_device_eval_batch_size 3 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --use_peft True \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --max_train_samples 20000 \\\n",
    "    --max_eval_samples 10 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 10 \\\n",
    "    --eval_steps 50 \\\n",
    "    --eval_strategy steps \\\n",
    "    --save_steps 500 \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 3 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --preprocessing_num_workers 1 \\\n",
    "    --block_size 128 \\\n",
    "    --output_dir outputs-pt \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ddp_timeout 30000 \\\n",
    "    --logging_first_step True \\\n",
    "    --target_modules all \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype bfloat16 \\\n",
    "    --device_map auto \\\n",
    "    --report_to tensorboard \\\n",
    "    --ddp_find_unused_parameters False \\\n",
    "    --gradient_checkpointing True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå¹¶PTæ¨¡å‹\n",
    "!python merge_peft_adapter.py \\\n",
    "    --base_model Qwen/Qwen2.5-0.5B \\\n",
    "    --lora_model outputs-pt \\\n",
    "    --output_dir merged-pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: åœ¨PTæ¨¡å‹ä¸ŠåšSFT\n",
    "!python supervised_finetuning.py \\\n",
    "    --model_name_or_path merged-pt \\\n",
    "    --train_file_dir ./data/finetune \\\n",
    "    --validation_file_dir ./data/finetune \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --use_peft True \\\n",
    "    --template_name qwen \\\n",
    "    --bf16 \\\n",
    "    --max_train_samples 1000 \\\n",
    "    --max_eval_samples 10 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --output_dir outputs-sft \\\n",
    "    --overwrite_output_dir \\\n",
    "    --target_modules all \\\n",
    "    --lora_rank 8 \\\n",
    "    --gradient_checkpointing True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå¹¶SFTæ¨¡å‹\n",
    "!python merge_peft_adapter.py \\\n",
    "    --base_model merged-pt \\\n",
    "    --lora_model outputs-sft \\\n",
    "    --output_dir merged-sft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ æ–¹æ¡ˆC: å®Œæ•´æµç¨‹ï¼ˆPT + SFT + DPOï¼‰\n",
    "\n",
    "åœ¨SFTåŸºç¡€ä¸Šç»§ç»­åšåå¥½ä¼˜åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3: DPOè®­ç»ƒ\n",
    "# éœ€è¦å…ˆå®ŒæˆSFTè®­ç»ƒ\n",
    "\n",
    "!python dpo_training.py \\\n",
    "    --model_name_or_path ./merged-sft \\\n",
    "    --template_name qwen \\\n",
    "    --train_file_dir ./data/reward \\\n",
    "    --validation_file_dir ./data/reward \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --use_peft True \\\n",
    "    --max_train_samples 1000 \\\n",
    "    --max_eval_samples 100 \\\n",
    "    --max_steps 200 \\\n",
    "    --eval_steps 20 \\\n",
    "    --save_steps 100 \\\n",
    "    --max_source_length 512 \\\n",
    "    --max_target_length 512 \\\n",
    "    --output_dir outputs-dpo \\\n",
    "    --target_modules all \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype bfloat16 \\\n",
    "    --bf16 True \\\n",
    "    --fp16 False \\\n",
    "    --device_map auto \\\n",
    "    --report_to tensorboard \\\n",
    "    --remove_unused_columns False \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --cache_dir ./cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå¹¶DPOæ¨¡å‹\n",
    "!python merge_peft_adapter.py \\\n",
    "    --base_model merged-sft \\\n",
    "    --lora_model outputs-dpo \\\n",
    "    --output_dir merged-dpo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Part 4: æŸ¥çœ‹è®­ç»ƒæ—¥å¿—\n",
    "\n",
    "ä½¿ç”¨TensorBoardæŸ¥çœ‹è®­ç»ƒæ›²çº¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir outputs-sft/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Part 5: å¿«é€Ÿæµ‹è¯•\n",
    "\n",
    "è®­ç»ƒå®Œæˆåï¼Œå¿«é€Ÿæµ‹è¯•ä¸€ä¸‹æ¨¡å‹æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç®€å•æ¨ç†æµ‹è¯•\n",
    "!python inference.py \\\n",
    "    --base_model merged-sft \\\n",
    "    --template_name qwen \\\n",
    "    --interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æˆ–è€…ä½¿ç”¨Pythonä»£ç æµ‹è¯•\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_path = \"merged-sft\"  # æˆ– merged-dpo\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# æµ‹è¯•\n",
    "messages = [{\"role\": \"user\", \"content\": \"ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½\"}]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Part 6: ä¿å­˜å’Œä¸‹è½½æ¨¡å‹\n",
    "\n",
    "è®­ç»ƒå®Œæˆåï¼Œä¿å­˜æ¨¡å‹ä»¥ä¾¿åç»­ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ–¹å¼1: ä¸‹è½½LoRAæƒé‡ï¼ˆæ¨èï¼‰\n",
    "\n",
    "LoRAæƒé‡å¾ˆå°ï¼ˆ50-500MBï¼‰ï¼Œå®¹æ˜“ä¸‹è½½å’Œä¼ è¾“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹LoRAæƒé‡å¤§å°\n",
    "!du -sh outputs-sft/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰“åŒ…LoRAæƒé‡\n",
    "!zip -r my-sft-lora.zip outputs-sft/\n",
    "\n",
    "# ä¸‹è½½ï¼ˆä¼šè‡ªåŠ¨å¼¹å‡ºä¸‹è½½çª—å£ï¼‰\n",
    "from google.colab import files\n",
    "files.download('my-sft-lora.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ–¹å¼2: ä¿å­˜åˆ°Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŒ‚è½½Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤åˆ¶LoRAæƒé‡åˆ°Drive\n",
    "!mkdir -p /content/drive/MyDrive/MyMedicalGPT_models\n",
    "!cp -r outputs-sft /content/drive/MyDrive/MyMedicalGPT_models/\n",
    "\n",
    "# å¦‚æœè®­ç»ƒäº†DPOï¼Œä¹Ÿä¿å­˜\n",
    "# !cp -r outputs-dpo /content/drive/MyDrive/MyMedicalGPT_models/\n",
    "\n",
    "print(\"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ° Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ–¹å¼3: ä¸Šä¼ åˆ°HuggingFace Hubï¼ˆå¯é€‰ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…huggingface_hub\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "# ç™»å½•HuggingFaceï¼ˆéœ€è¦tokenï¼‰\n",
    "# ä» https://huggingface.co/settings/tokens è·å–token\n",
    "login()\n",
    "\n",
    "# ä¸Šä¼ æ¨¡å‹\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path=\"outputs-sft\",\n",
    "    repo_id=\"YOUR-USERNAME/my-medical-model\",  # æ”¹æˆä½ çš„ç”¨æˆ·å\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ æ¨¡å‹å·²ä¸Šä¼ åˆ° HuggingFace Hub!\")\n",
    "print(\"è®¿é—®: https://huggingface.co/YOUR-USERNAME/my-medical-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Part 7: åœ¨æœ¬åœ°ä½¿ç”¨æ¨¡å‹\n",
    "\n",
    "ä¸‹è½½æ¨¡å‹åï¼Œåœ¨æœ¬åœ°è¿™æ ·ä½¿ç”¨ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# è§£å‹LoRAæƒé‡\n",
    "unzip my-sft-lora.zip\n",
    "\n",
    "# æ¨ç†ï¼ˆä¼šè‡ªåŠ¨ä¸‹è½½åŸºåº§æ¨¡å‹ï¼‰\n",
    "cd MyMedicalGPT\n",
    "python inference.py \\\n",
    "    --base_model Qwen/Qwen2.5-0.5B-Instruct \\\n",
    "    --lora_model outputs-sft \\\n",
    "    --template_name qwen \\\n",
    "    --interactive\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ å®Œæˆï¼\n",
    "\n",
    "### ä¸‹ä¸€æ­¥å¯ä»¥åšä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "1. **éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ**\n",
    "   - æœ¬åœ°GPUæ¨ç†\n",
    "   - äº‘æœåŠ¡å™¨éƒ¨ç½²\n",
    "   - vLLMé«˜æ€§èƒ½éƒ¨ç½²\n",
    "\n",
    "2. **ç»§ç»­ä¼˜åŒ–**\n",
    "   - ä½¿ç”¨æ›´å¤šæ•°æ®è®­ç»ƒ\n",
    "   - è°ƒæ•´è¶…å‚æ•°\n",
    "   - å°è¯•æ›´å¤§çš„æ¨¡å‹\n",
    "\n",
    "3. **è¯„ä¼°æ¨¡å‹**\n",
    "   - å‡†å¤‡æµ‹è¯•é›†\n",
    "   - è®¡ç®—BLEU/ROUGEç­‰æŒ‡æ ‡\n",
    "   - äººå·¥è¯„ä¼°\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š å‚è€ƒèµ„æº\n",
    "\n",
    "- [é¡¹ç›®README](../README.md)\n",
    "- [å¿«é€Ÿå¼€å§‹æŒ‡å—](../QUICKSTART.md)\n",
    "- [æ¨ç†æŠ€æœ¯è¯¦è§£](../INFERENCE_GUIDE.md)\n",
    "\n",
    "ç¥è®­ç»ƒæ„‰å¿«ï¼ğŸš€"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
