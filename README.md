# MyMedicalGPT - ç²¾ç®€ç‰ˆLLMè®­ç»ƒæ¡†æ¶

ä¸€ä¸ªç²¾ç®€çš„å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼ŒåŒ…å«å®Œæ•´çš„è®­ç»ƒå’Œæ¨ç†æµç¨‹ã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
MyMedicalGPT/
â”œâ”€â”€ README.md                    # é¡¹ç›®æ–‡æ¡£
â”œâ”€â”€ requirements.txt             # ä¾èµ–åŒ…
â”œâ”€â”€ pretraining.py              # é¢„è®­ç»ƒè„šæœ¬
â”œâ”€â”€ supervised_finetuning.py    # SFTå¾®è°ƒè„šæœ¬
â”œâ”€â”€ dpo_training.py             # DPOè®­ç»ƒè„šæœ¬
â”œâ”€â”€ merge_peft_adapter.py       # LoRAåˆå¹¶è„šæœ¬
â”œâ”€â”€ template.py                 # å¯¹è¯æ¨¡æ¿
â”œâ”€â”€ inference.py                # åŸºç¡€æ¨ç†
â”œâ”€â”€ inference_api.py            # APIæ¨ç†æœåŠ¡
â”œâ”€â”€ inference_gradio.py         # Gradio Webç•Œé¢
â”œâ”€â”€ inference_vllm.py           # vLLMé«˜æ€§èƒ½æ¨ç†
â”œâ”€â”€ scripts/                    # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_pt.sh            # é¢„è®­ç»ƒ
â”‚   â”œâ”€â”€ train_sft.sh           # SFTå¾®è°ƒ
â”‚   â”œâ”€â”€ train_dpo.sh           # DPOè®­ç»ƒ
â”‚   â”œâ”€â”€ merge_lora.sh          # åˆå¹¶LoRA
â”‚   â”œâ”€â”€ run_pipeline.sh        # å®Œæ•´æµç¨‹
â”‚   â”œâ”€â”€ inference_basic.sh     # åŸºç¡€æ¨ç†
â”‚   â”œâ”€â”€ inference_batch.sh     # æ‰¹é‡æ¨ç†
â”‚   â””â”€â”€ serve_vllm.sh          # vLLMæœåŠ¡
â”œâ”€â”€ data/                       # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ pretrain/              # é¢„è®­ç»ƒæ•°æ®(.txt)
â”‚   â”œâ”€â”€ finetune/              # SFTæ•°æ®(.jsonl)
â”‚   â””â”€â”€ reward/                # DPOæ•°æ®(.jsonl)
â”œâ”€â”€ configs/                    # é…ç½®æ–‡ä»¶
â””â”€â”€ notebooks/                  # Jupyter notebooks
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹

```bash
bash scripts/run_pipeline.sh
```

è¿™ä¼šä¾æ¬¡æ‰§è¡Œï¼š
1. **Stage 1: å¢é‡é¢„è®­ç»ƒ (PT)** - åœ¨é¢†åŸŸæ–‡æœ¬ä¸Šç»§ç»­è®­ç»ƒ
2. **Stage 2: æœ‰ç›‘ç£å¾®è°ƒ (SFT)** - ä½¿ç”¨æŒ‡ä»¤æ•°æ®å¯¹é½
3. **Stage 3: ç›´æ¥åå¥½ä¼˜åŒ– (DPO)** - ä»äººç±»åå¥½å­¦ä¹ 

### 3. å•ç‹¬è¿è¡ŒæŸä¸ªé˜¶æ®µ

```bash
# åªè¿è¡Œé¢„è®­ç»ƒ
bash scripts/train_pt.sh

# åªè¿è¡ŒSFT
bash scripts/train_sft.sh

# åªè¿è¡ŒDPO
bash scripts/train_dpo.sh
```

## ğŸ’¾ æ•°æ®æ ¼å¼

### é¢„è®­ç»ƒæ•°æ® (.txt)
çº¯æ–‡æœ¬æ–‡ä»¶ï¼Œä¸€è¡Œä¸€æ®µæˆ–ä¸€ç¯‡æ–‡æ¡£ï¼š
```
è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡æœ¬å†…å®¹
è¿™æ˜¯ç¬¬äºŒæ®µæ–‡æœ¬å†…å®¹
```

### SFTæ•°æ® (.jsonl)
ShareGPTæ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼š
```json
{
  "conversations": [
    {"from": "human", "value": "ä½ å¥½"},
    {"from": "gpt", "value": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}
  ]
}
```

### DPOæ•°æ® (.jsonl)
åå¥½å¯¹æ¯”æ ¼å¼ï¼š
```json
{
  "system": "",
  "question": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
  "response_chosen": "äººå·¥æ™ºèƒ½(AI)æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯...",
  "response_rejected": "AIå°±æ˜¯æœºå™¨äººã€‚"
}
```

## ğŸ¯ æ¨ç†ä½¿ç”¨

### æ–¹å¼1: å‘½ä»¤è¡Œäº¤äº’

```bash
python inference.py \
    --base_model merged-dpo \
    --template_name qwen \
    --interactive
```

### æ–¹å¼2: FastAPIæœåŠ¡

```bash
# å¯åŠ¨APIæœåŠ¡
python inference_api.py --model_path merged-dpo --port 8000

# ä½¿ç”¨curlæµ‹è¯•
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
    "history": []
  }'
```

### æ–¹å¼3: Gradio Webç•Œé¢

```bash
python inference_gradio.py --model_path merged-dpo --port 7860
```

ç„¶åè®¿é—® http://localhost:7860

### æ–¹å¼4: æ‰¹é‡æ¨ç†

```bash
# å‡†å¤‡è¾“å…¥æ–‡ä»¶ queries.jsonl
echo '{"query": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ?"}' > queries.jsonl
echo '{"query": "æ·±åº¦å­¦ä¹ æœ‰å“ªäº›åº”ç”¨?"}' >> queries.jsonl

# æ‰¹é‡æ¨ç†
python inference.py \
    --base_model merged-dpo \
    --data_file queries.jsonl \
    --output_file predictions.jsonl
```

### æ–¹å¼5: vLLM é«˜æ€§èƒ½æ¨ç† (æ¨èç”Ÿäº§ç¯å¢ƒ)

vLLM æä¾› 10-20x çš„ååé‡æå‡ï¼Œé€‚åˆç”Ÿäº§éƒ¨ç½²ã€‚

```bash
# å®‰è£… vLLM (éœ€è¦ Linux + NVIDIA GPU)
pip install vllm

# æ–¹å¼A: äº¤äº’å¼å¯¹è¯
python inference_vllm.py --model_path merged-dpo --interactive

# æ–¹å¼B: å¯åŠ¨ OpenAI å…¼å®¹ API æœåŠ¡
python inference_vllm.py --model_path merged-dpo --serve --port 8000

# æ–¹å¼C: æ‰¹é‡æ¨ç†
python inference_vllm.py \
    --model_path merged-dpo \
    --data_file queries.jsonl \
    --output_file vllm_output.jsonl
```

**è°ƒç”¨ vLLM API æœåŠ¡:**

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPTY"
)

response = client.chat.completions.create(
    model="merged-dpo",
    messages=[{"role": "user", "content": "ä½ å¥½"}]
)
print(response.choices[0].message.content)
```

**vLLM é«˜çº§é…ç½®:**

```bash
# å¤šå¡å¹¶è¡Œ
python inference_vllm.py --model_path merged-dpo --serve \
    --tensor_parallel_size 2

# AWQ é‡åŒ–
python inference_vllm.py --model_path merged-dpo --serve \
    --quantization awq

# è°ƒæ•´æ˜¾å­˜ä½¿ç”¨
python inference_vllm.py --model_path merged-dpo --serve \
    --gpu_memory_utilization 0.8
```

## âš™ï¸ è®­ç»ƒå‚æ•°è¯´æ˜

### æ ¸å¿ƒå‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `--model_name_or_path` | åŸºåº§æ¨¡å‹è·¯å¾„ | `Qwen/Qwen2.5-0.5B` |
| `--use_peft` | æ˜¯å¦ä½¿ç”¨LoRA | `True` |
| `--lora_rank` | LoRAç§© | 8-64 |
| `--learning_rate` | å­¦ä¹ ç‡ | PT: 2e-4, SFT: 2e-5 |
| `--num_train_epochs` | è®­ç»ƒè½®æ•° | 1-3 |
| `--per_device_train_batch_size` | æ‰¹æ¬¡å¤§å° | 2-8 |
| `--gradient_accumulation_steps` | æ¢¯åº¦ç´¯ç§¯ | 4-8 |

### æ˜¾å­˜ä¼˜åŒ–

- **ä½¿ç”¨LoRA**: `--use_peft True` (æ˜¾å­˜é™ä½80%)
- **æ¢¯åº¦æ£€æŸ¥ç‚¹**: `--gradient_checkpointing True` (æ˜¾å­˜é™ä½30%)
- **å‡å°batch size**: `--per_device_train_batch_size 2`
- **å¢åŠ æ¢¯åº¦ç´¯ç§¯**: `--gradient_accumulation_steps 8`
- **ä½¿ç”¨bf16**: `--bf16 True`

## ğŸ“Š æ¨¡å‹è¯„ä¼°

è®­ç»ƒæ—¥å¿—ä¿å­˜åœ¨ `outputs-*/runs/`ï¼Œä½¿ç”¨TensorBoardæŸ¥çœ‹ï¼š

```bash
tensorboard --logdir outputs-sft/runs --port 6006
```

## ğŸ”§ å¸¸è§é—®é¢˜

### 1. æ˜¾å­˜ä¸è¶³
- å‡å° `batch_size`
- å¢åŠ  `gradient_accumulation_steps`
- ä½¿ç”¨ `--gradient_checkpointing`
- å‡å° `block_size` æˆ– `max_length`

### 2. è®­ç»ƒé€Ÿåº¦æ…¢
- å¢å¤§ `batch_size`
- å‡å°‘ `gradient_accumulation_steps`
- å…³é—­ `--gradient_checkpointing`
- ä½¿ç”¨å¤šå¡è®­ç»ƒ

### 3. æ¨¡å‹æ•ˆæœå·®
- å¢åŠ è®­ç»ƒæ•°æ®é‡
- è°ƒæ•´å­¦ä¹ ç‡
- å¢åŠ è®­ç»ƒè½®æ•°
- æ£€æŸ¥æ•°æ®è´¨é‡

## ğŸ“ è‡ªå®šä¹‰ä¿®æ”¹æŒ‡å—

### æ·»åŠ è‡ªå·±çš„æ•°æ®
1. å°†æ•°æ®è½¬æ¢ä¸ºå¯¹åº”æ ¼å¼
2. æ”¾å…¥ `data/` å¯¹åº”ç›®å½•
3. ä¿®æ”¹è®­ç»ƒè„šæœ¬ä¸­çš„ `--train_file_dir`

### æ›´æ¢åŸºåº§æ¨¡å‹
1. ä¿®æ”¹ `--model_name_or_path`
2. è°ƒæ•´ `--template_name` (vicuna/alpaca/qwenç­‰)
3. æ ¹æ®æ¨¡å‹è°ƒæ•´ `--target_modules`

### è°ƒæ•´è®­ç»ƒç­–ç•¥
ç¼–è¾‘ `scripts/train_*.sh`ï¼Œä¿®æ”¹å‚æ•°ï¼š
- å­¦ä¹ ç‡
- batch size
- è®­ç»ƒè½®æ•°
- LoRAé…ç½®

## ğŸ“ å­¦ä¹ èµ„æº

- [HuggingFace Transformersæ–‡æ¡£](https://huggingface.co/docs/transformers)
- [PEFT (LoRA) æ–‡æ¡£](https://huggingface.co/docs/peft)
- [TRL (RLHF) æ–‡æ¡£](https://huggingface.co/docs/trl)

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ª Apache 2.0 è®¸å¯è¯ã€‚
