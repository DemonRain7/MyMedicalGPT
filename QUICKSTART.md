# å¿«é€Ÿå¼€å§‹æŒ‡å— - 5åˆ†é’Ÿä¸Šæ‰‹

## ğŸ¯ ç›®æ ‡
è¿™ä»½æŒ‡å—å¸®åŠ©ä½ åœ¨5åˆ†é’Ÿå†…å®Œæˆç¬¬ä¸€æ¬¡è®­ç»ƒå’Œæ¨ç†ã€‚

## æ­¥éª¤1: ç¯å¢ƒå‡†å¤‡ (1åˆ†é’Ÿ)

```bash
# å…‹éš†æˆ–å¤åˆ¶MyMedicalGPTæ–‡ä»¶å¤¹
cd MyMedicalGPT

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

## æ­¥éª¤2: å‡†å¤‡æ•°æ® (å¯é€‰)

é¡¹ç›®å·²åŒ…å«ç¤ºä¾‹æ•°æ®ï¼Œä½äº `data/` ç›®å½•ï¼š
- `data/pretrain/` - é¢„è®­ç»ƒæ–‡æœ¬
- `data/finetune/` - SFTæŒ‡ä»¤æ•°æ®
- `data/reward/` - DPOåå¥½æ•°æ®

å¦‚æœè¦ä½¿ç”¨è‡ªå·±çš„æ•°æ®ï¼Œå‚è€ƒè¿™äº›æ–‡ä»¶çš„æ ¼å¼ã€‚

## æ­¥éª¤3: åªè®­ç»ƒSFT (æ¨èæ–°æ‰‹)

å¦‚æœä½ æ˜¯ç¬¬ä¸€æ¬¡å°è¯•ï¼Œå»ºè®®è·³è¿‡PTé˜¶æ®µï¼Œç›´æ¥åšSFTï¼š

```bash
# ä½¿ç”¨Qwen2.5-0.5BåŸºåº§æ¨¡å‹ç›´æ¥åšSFT
python supervised_finetuning.py \
    --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
    --train_file_dir ./data/finetune \
    --validation_file_dir ./data/finetune \
    --per_device_train_batch_size 4 \
    --do_train \
    --use_peft True \
    --template_name qwen \
    --num_train_epochs 1 \
    --learning_rate 2e-5 \
    --output_dir outputs-sft \
    --bf16 \
    --target_modules all \
    --lora_rank 8 \
    --gradient_checkpointing True
```

è¿™ä¼šåœ¨10-30åˆ†é’Ÿå†…å®Œæˆï¼ˆå–å†³äºä½ çš„GPUï¼‰ã€‚

## æ­¥éª¤4: åˆå¹¶æ¨¡å‹

```bash
python merge_peft_adapter.py \
    --base_model Qwen/Qwen2.5-0.5B-Instruct \
    --lora_model outputs-sft \
    --output_dir my-first-model
```

## æ­¥éª¤5: æµ‹è¯•æ¨ç†

### æ–¹å¼A: å‘½ä»¤è¡Œäº¤äº’

```bash
python inference.py \
    --base_model my-first-model \
    --template_name qwen \
    --interactive
```

ç„¶åè¾“å…¥é—®é¢˜æµ‹è¯•ï¼š
```
USER: ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½
ASSISTANT: ...
```

### æ–¹å¼B: Webç•Œé¢ (æ›´å‹å¥½)

```bash
# å…ˆå®‰è£…gradio
pip install gradio

# å¯åŠ¨ç•Œé¢
python inference_gradio.py --model_path my-first-model
```

è®¿é—® http://localhost:7860

## ğŸ‰ æ­å–œï¼

ä½ å·²ç»å®Œæˆäº†ç¬¬ä¸€ä¸ªæ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†ï¼

## ä¸‹ä¸€æ­¥åšä»€ä¹ˆï¼Ÿ

### 1. å®Œæ•´è®­ç»ƒæµç¨‹ (PT -> SFT -> DPO)

```bash
bash scripts/run_pipeline.sh
```

è¿™ä¼šå®Œæ•´èµ°ä¸€éä¸‰ä¸ªé˜¶æ®µã€‚

### 2. ä½¿ç”¨è‡ªå·±çš„æ•°æ®

**SFTæ•°æ®æ ¼å¼** (`data/finetune/my_data.jsonl`):
```json
{"conversations": [
  {"from": "human", "value": "é—®é¢˜1"},
  {"from": "gpt", "value": "å›ç­”1"}
]}
```

ç„¶åä¿®æ”¹è®­ç»ƒè„šæœ¬çš„ `--train_file_dir`ã€‚

### 3. è°ƒæ•´è®­ç»ƒå‚æ•°

ç¼–è¾‘ `scripts/train_sft.sh`ï¼Œä¿®æ”¹ï¼š
- `--num_train_epochs` - è®­ç»ƒè½®æ•°
- `--learning_rate` - å­¦ä¹ ç‡
- `--lora_rank` - LoRAç§©(è¶Šå¤§æ•ˆæœè¶Šå¥½ï¼Œæ˜¾å­˜è¶Šå¤§)

### 4. éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ

å‚è€ƒ [INFERENCE_GUIDE.md](INFERENCE_GUIDE.md) äº†è§£vLLMç­‰é«˜æ€§èƒ½éƒ¨ç½²æ–¹æ¡ˆã€‚

## å¸¸è§é—®é¢˜

### Q: æ˜¾å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ
A: å‡å°batch sizeæˆ–ä½¿ç”¨é‡åŒ–ï¼š
```bash
python inference.py \
    --base_model my-first-model \
    --load_in_8bit  # æ˜¾å­˜å‡åŠ
```

### Q: è®­ç»ƒå¤ªæ…¢ï¼Ÿ
A:
- å‡å°æ•°æ®é›†: `--max_train_samples 1000`
- å‡å°‘epoch: `--num_train_epochs 1`
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹

### Q: æ¨¡å‹æ•ˆæœä¸å¥½ï¼Ÿ
A:
- å¢åŠ è®­ç»ƒæ•°æ®
- æé«˜è®­ç»ƒè½®æ•°
- æ£€æŸ¥æ•°æ®è´¨é‡
- å°è¯•æ›´å¤§çš„åŸºåº§æ¨¡å‹

### Q: å¦‚ä½•è¯„ä¼°æ¨¡å‹ï¼Ÿ
A:
```bash
# åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
python supervised_finetuning.py \
    --model_name_or_path my-first-model \
    --validation_file_dir ./data/finetune \
    --do_eval \
    --per_device_eval_batch_size 4
```

## å­¦ä¹ èµ„æº

- [README.md](README.md) - å®Œæ•´é¡¹ç›®æ–‡æ¡£
- [INFERENCE_GUIDE.md](INFERENCE_GUIDE.md) - æ¨ç†æŠ€æœ¯è¯¦è§£

ç¥ä½ è®­ç»ƒæ„‰å¿«! ğŸš€
